{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The overall goal is to create a Python script that makes a custom model for emotional recognition and deploys it live to a camera.\n",
    "- The structure of the data is a subset of folders containing images. The folders are labeled by their respective motions: Ahegao, Angry, Happy, Neutral, Sad, Surprise. \n",
    "- Convert the RGB images into a numpy array.\n",
    "- When creating the CNN architecture, add Ridge Regression regularization to each layer. Add Dropout and BatchNormalization layers.\n",
    "- When training the CNN add Kfold cross validation.\n",
    "- Make sure the script is using an Nvidia GPU\n",
    "- Save the best performing checkpoint and label the model as “emotional_recognition_val_accuracy_{val_accuracy:.2f}”\n",
    "- Then write a script in Python, referring to the prior script, that pre processes the live feed of the \n",
    "\n",
    "**More Details Here**\n",
    "https://chat.openai.com/share/6aca2c47-efd4-41a6-83cd-9eabe7b2d4fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Best hyperparameters: {'batch_size': 16, 'dropout_val': 0.3, 'epochs': 10, 'l2_val': 0.001, 'num_folds': 10}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with hyperparameters: {'batch_size': 16, 'dropout_val': 0.7, 'epochs': 5, 'l2_val': 0.1, 'num_folds': 2}\n",
      "Epoch 1/5\n",
      "1122/1122 [==============================] - 14s 8ms/step - loss: 2.6625 - accuracy: 0.2748 - val_loss: 1.8729 - val_accuracy: 0.2553\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25529, saving model to emotional_CNN_val_accuracy_0.26.h5\n",
      "Epoch 2/5\n",
      "1122/1122 [==============================] - 9s 8ms/step - loss: 1.8101 - accuracy: 0.2972 - val_loss: 1.8366 - val_accuracy: 0.2859\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.25529 to 0.28589, saving model to emotional_CNN_val_accuracy_0.29.h5\n",
      "Epoch 3/5\n",
      "1122/1122 [==============================] - 9s 8ms/step - loss: 1.8046 - accuracy: 0.3001 - val_loss: 1.8294 - val_accuracy: 0.3001\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.28589 to 0.30010, saving model to emotional_CNN_val_accuracy_0.30.h5\n",
      "Epoch 4/5\n",
      "1122/1122 [==============================] - 10s 9ms/step - loss: 1.7875 - accuracy: 0.3121 - val_loss: 1.9648 - val_accuracy: 0.2778\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.30010\n",
      "Epoch 5/5\n",
      "1122/1122 [==============================] - 9s 8ms/step - loss: 1.7808 - accuracy: 0.3117 - val_loss: 1.9582 - val_accuracy: 0.2622\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.30010\n",
      "Epoch 1/5\n",
      "1122/1122 [==============================] - 10s 8ms/step - loss: 2.6672 - accuracy: 0.2740 - val_loss: 2.0356 - val_accuracy: 0.2541\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25414, saving model to emotional_CNN_val_accuracy_0.25.h5\n",
      "Epoch 2/5\n",
      "1122/1122 [==============================] - 9s 8ms/step - loss: 1.8098 - accuracy: 0.2968 - val_loss: 1.8723 - val_accuracy: 0.1986\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.25414\n",
      "Epoch 3/5\n",
      "1122/1122 [==============================] - 8s 7ms/step - loss: 1.7877 - accuracy: 0.3034 - val_loss: 1.7611 - val_accuracy: 0.3035\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.25414 to 0.30346, saving model to emotional_CNN_val_accuracy_0.30.h5\n",
      "Epoch 4/5\n",
      "1122/1122 [==============================] - 7s 7ms/step - loss: 1.7808 - accuracy: 0.3100 - val_loss: 2.0170 - val_accuracy: 0.2033\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.30346\n",
      "Epoch 5/5\n",
      "1122/1122 [==============================] - 7s 7ms/step - loss: 1.7736 - accuracy: 0.3106 - val_loss: 1.7816 - val_accuracy: 0.3015\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.30346\n",
      "Training with hyperparameters: {'batch_size': 16, 'dropout_val': 0.7, 'epochs': 5, 'l2_val': 0.1, 'num_folds': 5}\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.4037 - accuracy: 0.2856 - val_loss: 2.9792 - val_accuracy: 0.2516\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25160, saving model to emotional_CNN_val_accuracy_0.25.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.8045 - accuracy: 0.3124 - val_loss: 1.8177 - val_accuracy: 0.3070\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.25160 to 0.30705, saving model to emotional_CNN_val_accuracy_0.31.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7877 - accuracy: 0.3242 - val_loss: 2.0273 - val_accuracy: 0.2119\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.30705\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7813 - accuracy: 0.3277 - val_loss: 1.7979 - val_accuracy: 0.3181\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.30705 to 0.31806, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7815 - accuracy: 0.3296 - val_loss: 2.1081 - val_accuracy: 0.2717\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.31806\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.2902 - accuracy: 0.2606 - val_loss: 1.8037 - val_accuracy: 0.2796\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.27960, saving model to emotional_CNN_val_accuracy_0.28.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7893 - accuracy: 0.2894 - val_loss: 1.7877 - val_accuracy: 0.2979\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.27960 to 0.29785, saving model to emotional_CNN_val_accuracy_0.30.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7782 - accuracy: 0.2953 - val_loss: 1.7386 - val_accuracy: 0.3172\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.29785 to 0.31722, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7705 - accuracy: 0.2993 - val_loss: 1.7960 - val_accuracy: 0.2902\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.31722\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7641 - accuracy: 0.3019 - val_loss: 1.7514 - val_accuracy: 0.3189\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.31722 to 0.31889, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.3838 - accuracy: 0.2630 - val_loss: 1.8200 - val_accuracy: 0.2834\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.28341, saving model to emotional_CNN_val_accuracy_0.28.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.8124 - accuracy: 0.3015 - val_loss: 2.3111 - val_accuracy: 0.1998\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.28341\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7921 - accuracy: 0.3138 - val_loss: 2.5458 - val_accuracy: 0.1689\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.28341\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7766 - accuracy: 0.3219 - val_loss: 1.8618 - val_accuracy: 0.2993\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.28341 to 0.29929, saving model to emotional_CNN_val_accuracy_0.30.h5\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7721 - accuracy: 0.3246 - val_loss: 1.7325 - val_accuracy: 0.3513\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.29929 to 0.35126, saving model to emotional_CNN_val_accuracy_0.35.h5\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.3792 - accuracy: 0.2770 - val_loss: 2.0033 - val_accuracy: 0.1534\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.15341, saving model to emotional_CNN_val_accuracy_0.15.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.8047 - accuracy: 0.3052 - val_loss: 1.7577 - val_accuracy: 0.3219\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.15341 to 0.32186, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7884 - accuracy: 0.3166 - val_loss: 2.6959 - val_accuracy: 0.1155\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.32186\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7783 - accuracy: 0.3212 - val_loss: 2.6572 - val_accuracy: 0.2554\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.32186\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7739 - accuracy: 0.3208 - val_loss: 2.3735 - val_accuracy: 0.2569\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.32186\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 13s 6ms/step - loss: 2.3215 - accuracy: 0.2818 - val_loss: 2.0085 - val_accuracy: 0.2475\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.24746, saving model to emotional_CNN_val_accuracy_0.25.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 10s 5ms/step - loss: 1.7954 - accuracy: 0.3075 - val_loss: 1.7451 - val_accuracy: 0.3203\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.24746 to 0.32033, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.7753 - accuracy: 0.3170 - val_loss: 2.1806 - val_accuracy: 0.1701\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.32033\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 10s 5ms/step - loss: 1.7759 - accuracy: 0.3175 - val_loss: 2.1782 - val_accuracy: 0.1777\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.32033\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 10s 5ms/step - loss: 1.7666 - accuracy: 0.3242 - val_loss: 1.9641 - val_accuracy: 0.2909\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.32033\n",
      "Training with hyperparameters: {'batch_size': 16, 'dropout_val': 0.7, 'epochs': 5, 'l2_val': 0.1, 'num_folds': 10}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.3387 - accuracy: 0.2957 - val_loss: 2.3344 - val_accuracy: 0.1226\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12260, saving model to emotional_CNN_val_accuracy_0.12.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 10s 5ms/step - loss: 1.8042 - accuracy: 0.3284 - val_loss: 2.0315 - val_accuracy: 0.3082\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.12260 to 0.30816, saving model to emotional_CNN_val_accuracy_0.31.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 11s 5ms/step - loss: 1.7825 - accuracy: 0.3352 - val_loss: 2.2427 - val_accuracy: 0.1669\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.30816\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 11s 5ms/step - loss: 1.7773 - accuracy: 0.3390 - val_loss: 1.8338 - val_accuracy: 0.3051\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.30816\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 11s 5ms/step - loss: 1.7771 - accuracy: 0.3360 - val_loss: 3.8843 - val_accuracy: 0.1140\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.30816\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 11s 5ms/step - loss: 2.3040 - accuracy: 0.2951 - val_loss: 1.8539 - val_accuracy: 0.3330\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.33296, saving model to emotional_CNN_val_accuracy_0.33.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 10s 5ms/step - loss: 1.7982 - accuracy: 0.3139 - val_loss: 2.0251 - val_accuracy: 0.2171\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.33296\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 10s 5ms/step - loss: 1.7755 - accuracy: 0.3311 - val_loss: 1.9635 - val_accuracy: 0.2210\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.33296\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 10s 5ms/step - loss: 1.7689 - accuracy: 0.3312 - val_loss: 2.2987 - val_accuracy: 0.2742\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.33296\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 5ms/step - loss: 1.7674 - accuracy: 0.3293 - val_loss: 2.7724 - val_accuracy: 0.3098\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.33296\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 5ms/step - loss: 2.2728 - accuracy: 0.2824 - val_loss: 2.0324 - val_accuracy: 0.2502\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25021, saving model to emotional_CNN_val_accuracy_0.25.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7939 - accuracy: 0.3045 - val_loss: 2.6477 - val_accuracy: 0.1683\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.25021\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7826 - accuracy: 0.3198 - val_loss: 2.0101 - val_accuracy: 0.2313\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.25021\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7771 - accuracy: 0.3190 - val_loss: 4.1351 - val_accuracy: 0.2474\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.25021\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7668 - accuracy: 0.3242 - val_loss: 2.3063 - val_accuracy: 0.2488\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.25021\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.2527 - accuracy: 0.2793 - val_loss: 2.3996 - val_accuracy: 0.1421\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.14210, saving model to emotional_CNN_val_accuracy_0.14.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7899 - accuracy: 0.3109 - val_loss: 2.0065 - val_accuracy: 0.2644\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.14210 to 0.26442, saving model to emotional_CNN_val_accuracy_0.26.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7777 - accuracy: 0.3263 - val_loss: 1.7893 - val_accuracy: 0.3090\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.26442 to 0.30900, saving model to emotional_CNN_val_accuracy_0.31.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7728 - accuracy: 0.3237 - val_loss: 1.8027 - val_accuracy: 0.3176\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.30900 to 0.31764, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7668 - accuracy: 0.3253 - val_loss: 1.7546 - val_accuracy: 0.3229\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.31764 to 0.32293, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.2551 - accuracy: 0.2816 - val_loss: 1.9297 - val_accuracy: 0.2143\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.21427, saving model to emotional_CNN_val_accuracy_0.21.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7872 - accuracy: 0.2947 - val_loss: 1.7519 - val_accuracy: 0.3235\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.21427 to 0.32349, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7701 - accuracy: 0.3029 - val_loss: 1.7361 - val_accuracy: 0.2962\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.32349\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7668 - accuracy: 0.3029 - val_loss: 1.8045 - val_accuracy: 0.2619\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.32349\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7595 - accuracy: 0.3063 - val_loss: 1.7240 - val_accuracy: 0.3475\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.32349 to 0.34745, saving model to emotional_CNN_val_accuracy_0.35.h5\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 10s 4ms/step - loss: 2.2471 - accuracy: 0.2764 - val_loss: 1.7865 - val_accuracy: 0.3246\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.32460, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7934 - accuracy: 0.3001 - val_loss: 2.1735 - val_accuracy: 0.1535\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.32460\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7746 - accuracy: 0.3103 - val_loss: 1.8585 - val_accuracy: 0.2218\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.32460\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7709 - accuracy: 0.3140 - val_loss: 1.7850 - val_accuracy: 0.3082\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.32460\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7655 - accuracy: 0.3207 - val_loss: 2.0023 - val_accuracy: 0.2722\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.32460\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.2867 - accuracy: 0.2717 - val_loss: 1.8182 - val_accuracy: 0.2956\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.29563, saving model to emotional_CNN_val_accuracy_0.30.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7922 - accuracy: 0.2940 - val_loss: 2.1623 - val_accuracy: 0.2483\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.29563\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7815 - accuracy: 0.2999 - val_loss: 1.8344 - val_accuracy: 0.2524\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.29563\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7709 - accuracy: 0.3082 - val_loss: 1.7384 - val_accuracy: 0.3533\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.29563 to 0.35330, saving model to emotional_CNN_val_accuracy_0.35.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7678 - accuracy: 0.3077 - val_loss: 2.1128 - val_accuracy: 0.1257\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.35330\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.2723 - accuracy: 0.2691 - val_loss: 1.7813 - val_accuracy: 0.3177\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.31773, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7778 - accuracy: 0.3017 - val_loss: 1.7432 - val_accuracy: 0.3314\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.31773 to 0.33138, saving model to emotional_CNN_val_accuracy_0.33.h5\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7697 - accuracy: 0.3055 - val_loss: 1.7556 - val_accuracy: 0.3216\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.33138\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7659 - accuracy: 0.3070 - val_loss: 1.7507 - val_accuracy: 0.2940\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.33138\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7646 - accuracy: 0.3093 - val_loss: 1.7585 - val_accuracy: 0.3180\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.33138\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.2955 - accuracy: 0.2502 - val_loss: 1.8280 - val_accuracy: 0.2506\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25056, saving model to emotional_CNN_val_accuracy_0.25.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7919 - accuracy: 0.2844 - val_loss: 1.7745 - val_accuracy: 0.3055\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.25056 to 0.30546, saving model to emotional_CNN_val_accuracy_0.31.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7740 - accuracy: 0.2982 - val_loss: 1.7819 - val_accuracy: 0.2890\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.30546\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7725 - accuracy: 0.2973 - val_loss: 1.7676 - val_accuracy: 0.2954\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.30546\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7678 - accuracy: 0.2986 - val_loss: 1.7961 - val_accuracy: 0.2690\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.30546\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.3042 - accuracy: 0.2867 - val_loss: 1.8484 - val_accuracy: 0.2834\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.28344, saving model to emotional_CNN_val_accuracy_0.28.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7944 - accuracy: 0.3162 - val_loss: 2.2432 - val_accuracy: 0.1544\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.28344\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7852 - accuracy: 0.3204 - val_loss: 1.7581 - val_accuracy: 0.3161\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.28344 to 0.31605, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7737 - accuracy: 0.3288 - val_loss: 2.4660 - val_accuracy: 0.2511\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.31605\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7689 - accuracy: 0.3320 - val_loss: 2.8383 - val_accuracy: 0.2584\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.31605\n",
      "Training with hyperparameters: {'batch_size': 16, 'dropout_val': 0.7, 'epochs': 5, 'l2_val': 0.01, 'num_folds': 2}\n",
      "Epoch 1/5\n",
      "1122/1122 [==============================] - 7s 6ms/step - loss: 2.1650 - accuracy: 0.3213 - val_loss: 1.7986 - val_accuracy: 0.3475\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.34747, saving model to emotional_CNN_val_accuracy_0.35.h5\n",
      "Epoch 2/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.7479 - accuracy: 0.3705 - val_loss: 2.1082 - val_accuracy: 0.2452\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.34747\n",
      "Epoch 3/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.7303 - accuracy: 0.3692 - val_loss: 1.6562 - val_accuracy: 0.4180\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.34747 to 0.41797, saving model to emotional_CNN_val_accuracy_0.42.h5\n",
      "Epoch 4/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.7085 - accuracy: 0.3766 - val_loss: 1.6418 - val_accuracy: 0.4004\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.41797\n",
      "Epoch 5/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.6937 - accuracy: 0.3919 - val_loss: 1.6751 - val_accuracy: 0.3918\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.41797\n",
      "Epoch 1/5\n",
      "1122/1122 [==============================] - 7s 6ms/step - loss: 2.2057 - accuracy: 0.3068 - val_loss: 1.8089 - val_accuracy: 0.3352\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.33523, saving model to emotional_CNN_val_accuracy_0.34.h5\n",
      "Epoch 2/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.7731 - accuracy: 0.3474 - val_loss: 2.0852 - val_accuracy: 0.2656\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.33523\n",
      "Epoch 3/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.7349 - accuracy: 0.3629 - val_loss: 1.8103 - val_accuracy: 0.3196\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.33523\n",
      "Epoch 4/5\n",
      "1122/1122 [==============================] - 6s 6ms/step - loss: 1.7109 - accuracy: 0.3696 - val_loss: 1.6311 - val_accuracy: 0.3801\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.33523 to 0.38009, saving model to emotional_CNN_val_accuracy_0.38.h5\n",
      "Epoch 5/5\n",
      "1122/1122 [==============================] - 7s 6ms/step - loss: 1.6894 - accuracy: 0.3806 - val_loss: 1.6307 - val_accuracy: 0.4148\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.38009 to 0.41476, saving model to emotional_CNN_val_accuracy_0.41.h5\n",
      "Training with hyperparameters: {'batch_size': 16, 'dropout_val': 0.7, 'epochs': 5, 'l2_val': 0.01, 'num_folds': 5}\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.0519 - accuracy: 0.3237 - val_loss: 2.4501 - val_accuracy: 0.1491\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.14907, saving model to emotional_CNN_val_accuracy_0.15.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7422 - accuracy: 0.3546 - val_loss: 1.8115 - val_accuracy: 0.3181\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.14907 to 0.31806, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7085 - accuracy: 0.3666 - val_loss: 1.9854 - val_accuracy: 0.2249\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.31806\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6886 - accuracy: 0.3829 - val_loss: 1.5836 - val_accuracy: 0.4404\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.31806 to 0.44037, saving model to emotional_CNN_val_accuracy_0.44.h5\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6809 - accuracy: 0.3894 - val_loss: 1.6873 - val_accuracy: 0.3851\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.44037\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.0717 - accuracy: 0.3150 - val_loss: 2.5243 - val_accuracy: 0.1211\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.12106, saving model to emotional_CNN_val_accuracy_0.12.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7645 - accuracy: 0.3379 - val_loss: 1.6797 - val_accuracy: 0.3695\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.12106 to 0.36946, saving model to emotional_CNN_val_accuracy_0.37.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7224 - accuracy: 0.3543 - val_loss: 1.8252 - val_accuracy: 0.3158\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.36946\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7041 - accuracy: 0.3668 - val_loss: 1.6874 - val_accuracy: 0.3636\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.36946\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6873 - accuracy: 0.3747 - val_loss: 1.6123 - val_accuracy: 0.3952\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.36946 to 0.39524, saving model to emotional_CNN_val_accuracy_0.40.h5\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 1.9954 - accuracy: 0.3223 - val_loss: 1.7429 - val_accuracy: 0.3539\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.35391, saving model to emotional_CNN_val_accuracy_0.35.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7306 - accuracy: 0.3594 - val_loss: 2.6239 - val_accuracy: 0.2658\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.35391\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7019 - accuracy: 0.3696 - val_loss: 1.7586 - val_accuracy: 0.3368\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.35391\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6823 - accuracy: 0.3770 - val_loss: 1.5643 - val_accuracy: 0.4415\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.35391 to 0.44155, saving model to emotional_CNN_val_accuracy_0.44.h5\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6758 - accuracy: 0.3808 - val_loss: 2.0026 - val_accuracy: 0.2327\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.44155\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.0786 - accuracy: 0.3007 - val_loss: 2.0374 - val_accuracy: 0.2284\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.22837, saving model to emotional_CNN_val_accuracy_0.23.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7582 - accuracy: 0.3371 - val_loss: 1.8026 - val_accuracy: 0.3031\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.22837 to 0.30305, saving model to emotional_CNN_val_accuracy_0.30.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7216 - accuracy: 0.3546 - val_loss: 1.6227 - val_accuracy: 0.4041\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.30305 to 0.40407, saving model to emotional_CNN_val_accuracy_0.40.h5\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7031 - accuracy: 0.3619 - val_loss: 1.6663 - val_accuracy: 0.3809\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.40407\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6840 - accuracy: 0.3722 - val_loss: 1.6902 - val_accuracy: 0.3524\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.40407\n",
      "Epoch 1/5\n",
      "1795/1795 [==============================] - 9s 5ms/step - loss: 2.0716 - accuracy: 0.3096 - val_loss: 3.3937 - val_accuracy: 0.1508\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.15076, saving model to emotional_CNN_val_accuracy_0.15.h5\n",
      "Epoch 2/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.7416 - accuracy: 0.3631 - val_loss: 1.6707 - val_accuracy: 0.3869\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.15076 to 0.38693, saving model to emotional_CNN_val_accuracy_0.39.h5\n",
      "Epoch 3/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6998 - accuracy: 0.3731 - val_loss: 1.6648 - val_accuracy: 0.3812\n",
      "\n",
      "Epoch 00003: val_accuracy did not improve from 0.38693\n",
      "Epoch 4/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6821 - accuracy: 0.3833 - val_loss: 1.6112 - val_accuracy: 0.4112\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.38693 to 0.41117, saving model to emotional_CNN_val_accuracy_0.41.h5\n",
      "Epoch 5/5\n",
      "1795/1795 [==============================] - 8s 5ms/step - loss: 1.6721 - accuracy: 0.3891 - val_loss: 1.6480 - val_accuracy: 0.3921\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.41117\n",
      "Training with hyperparameters: {'batch_size': 16, 'dropout_val': 0.7, 'epochs': 5, 'l2_val': 0.01, 'num_folds': 10}\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 10s 5ms/step - loss: 1.9989 - accuracy: 0.3281 - val_loss: 1.6545 - val_accuracy: 0.3773\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.37726, saving model to emotional_CNN_val_accuracy_0.38.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7268 - accuracy: 0.3632 - val_loss: 1.7840 - val_accuracy: 0.3318\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.37726\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7034 - accuracy: 0.3785 - val_loss: 1.6794 - val_accuracy: 0.3853\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.37726 to 0.38534, saving model to emotional_CNN_val_accuracy_0.39.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6874 - accuracy: 0.3856 - val_loss: 1.8618 - val_accuracy: 0.3196\n",
      "\n",
      "Epoch 00004: val_accuracy did not improve from 0.38534\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6809 - accuracy: 0.3875 - val_loss: 1.5798 - val_accuracy: 0.4132\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.38534 to 0.41321, saving model to emotional_CNN_val_accuracy_0.41.h5\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.0085 - accuracy: 0.3253 - val_loss: 1.6681 - val_accuracy: 0.3906\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.39064, saving model to emotional_CNN_val_accuracy_0.39.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7055 - accuracy: 0.3730 - val_loss: 1.7707 - val_accuracy: 0.3268\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.39064\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6818 - accuracy: 0.3854 - val_loss: 1.5890 - val_accuracy: 0.3982\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.39064 to 0.39816, saving model to emotional_CNN_val_accuracy_0.40.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6728 - accuracy: 0.3879 - val_loss: 1.6293 - val_accuracy: 0.4032\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.39816 to 0.40318, saving model to emotional_CNN_val_accuracy_0.40.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6626 - accuracy: 0.3953 - val_loss: 1.6270 - val_accuracy: 0.3898\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.40318\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.0432 - accuracy: 0.3160 - val_loss: 2.1806 - val_accuracy: 0.2290\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.22903, saving model to emotional_CNN_val_accuracy_0.23.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7308 - accuracy: 0.3622 - val_loss: 1.8278 - val_accuracy: 0.3229\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.22903 to 0.32293, saving model to emotional_CNN_val_accuracy_0.32.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7044 - accuracy: 0.3687 - val_loss: 1.6641 - val_accuracy: 0.3628\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.32293 to 0.36278, saving model to emotional_CNN_val_accuracy_0.36.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6788 - accuracy: 0.3804 - val_loss: 1.5823 - val_accuracy: 0.4249\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.36278 to 0.42491, saving model to emotional_CNN_val_accuracy_0.42.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6612 - accuracy: 0.3912 - val_loss: 1.6460 - val_accuracy: 0.3998\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.42491\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.0042 - accuracy: 0.3300 - val_loss: 2.3642 - val_accuracy: 0.2533\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.25327, saving model to emotional_CNN_val_accuracy_0.25.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7097 - accuracy: 0.3754 - val_loss: 1.9250 - val_accuracy: 0.3090\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.25327 to 0.30900, saving model to emotional_CNN_val_accuracy_0.31.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6903 - accuracy: 0.3842 - val_loss: 1.6301 - val_accuracy: 0.3934\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.30900 to 0.39342, saving model to emotional_CNN_val_accuracy_0.39.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6706 - accuracy: 0.3900 - val_loss: 1.6194 - val_accuracy: 0.4099\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.39342 to 0.40986, saving model to emotional_CNN_val_accuracy_0.41.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6663 - accuracy: 0.4009 - val_loss: 1.6897 - val_accuracy: 0.3845\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.40986\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.0380 - accuracy: 0.3240 - val_loss: 1.7089 - val_accuracy: 0.3842\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.38423, saving model to emotional_CNN_val_accuracy_0.38.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7308 - accuracy: 0.3716 - val_loss: 2.1318 - val_accuracy: 0.2750\n",
      "\n",
      "Epoch 00002: val_accuracy did not improve from 0.38423\n",
      "Epoch 3/5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7032 - accuracy: 0.3777 - val_loss: 1.6478 - val_accuracy: 0.3892\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.38423 to 0.38924, saving model to emotional_CNN_val_accuracy_0.39.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6933 - accuracy: 0.3866 - val_loss: 1.5856 - val_accuracy: 0.4589\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.38924 to 0.45890, saving model to emotional_CNN_val_accuracy_0.46.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6823 - accuracy: 0.3912 - val_loss: 1.6981 - val_accuracy: 0.3945\n",
      "\n",
      "Epoch 00005: val_accuracy did not improve from 0.45890\n",
      "Epoch 1/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 2.0651 - accuracy: 0.3272 - val_loss: 1.7789 - val_accuracy: 0.3633\n",
      "\n",
      "Epoch 00001: val_accuracy improved from -inf to 0.36333, saving model to emotional_CNN_val_accuracy_0.36.h5\n",
      "Epoch 2/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.7250 - accuracy: 0.3676 - val_loss: 1.7226 - val_accuracy: 0.3734\n",
      "\n",
      "Epoch 00002: val_accuracy improved from 0.36333 to 0.37336, saving model to emotional_CNN_val_accuracy_0.37.h5\n",
      "Epoch 3/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6911 - accuracy: 0.3844 - val_loss: 1.6821 - val_accuracy: 0.3823\n",
      "\n",
      "Epoch 00003: val_accuracy improved from 0.37336 to 0.38228, saving model to emotional_CNN_val_accuracy_0.38.h5\n",
      "Epoch 4/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6790 - accuracy: 0.3921 - val_loss: 1.6839 - val_accuracy: 0.4048\n",
      "\n",
      "Epoch 00004: val_accuracy improved from 0.38228 to 0.40485, saving model to emotional_CNN_val_accuracy_0.40.h5\n",
      "Epoch 5/5\n",
      "2019/2019 [==============================] - 9s 4ms/step - loss: 1.6653 - accuracy: 0.3970 - val_loss: 1.6394 - val_accuracy: 0.4185\n",
      "\n",
      "Epoch 00005: val_accuracy improved from 0.40485 to 0.41850, saving model to emotional_CNN_val_accuracy_0.42.h5\n"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-4e65daa16955>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    109\u001b[0m                                      save_best_only=True, mode='max')\n\u001b[0;32m    110\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n\u001b[0m\u001b[0;32m    112\u001b[0m                             validation_data=(X_val, y_val), callbacks=[checkpoint], verbose=1)\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1132\u001b[0m          \u001b[0mtraining_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRespectCompiledTrainableState\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1133\u001b[0m       \u001b[1;31m# Creates a `tf.data.Dataset` and handles batch and epoch iteration.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1134\u001b[1;33m       data_handler = data_adapter.get_data_handler(\n\u001b[0m\u001b[0;32m   1135\u001b[0m           \u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1136\u001b[0m           \u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1381\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1382\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1383\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1384\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1385\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1137\u001b[0m     \u001b[0madapter_cls\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mselect_data_adapter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1138\u001b[1;33m     self._adapter = adapter_cls(\n\u001b[0m\u001b[0;32m   1139\u001b[0m         \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1140\u001b[0m         \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    228\u001b[0m                **kwargs):\n\u001b[0;32m    229\u001b[0m     \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTensorLikeDataAdapter\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 230\u001b[1;33m     \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_tensorlike\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    231\u001b[0m     sample_weight_modes = broadcast_sample_weight_modes(\n\u001b[0;32m    232\u001b[0m         sample_weights, sample_weight_modes)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_process_tensorlike\u001b[1;34m(inputs)\u001b[0m\n\u001b[0;32m   1029\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1031\u001b[1;33m   \u001b[0minputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap_structure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_convert_numpy_and_scipy\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1032\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__internal__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_to_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1033\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36mmap_structure\u001b[1;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\nest.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m   return pack_sequence_as(\n\u001b[1;32m--> 869\u001b[1;33m       \u001b[0mstructure\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentries\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    870\u001b[0m       expand_composites=expand_composites)\n\u001b[0;32m    871\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m_convert_numpy_and_scipy\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m   1024\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0missubclass\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloating\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1025\u001b[0m         \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloatx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1026\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconvert_to_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1027\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0m_is_scipy_sparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1028\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_scipy_sparse_to_sparse_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2_with_dispatch\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1428\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[0mthe\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0ma\u001b[0m \u001b[0mtensor\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mof\u001b[0m \u001b[0mgiven\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;31m`\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgraph\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1429\u001b[0m   \"\"\"\n\u001b[1;32m-> 1430\u001b[1;33m   return convert_to_tensor_v2(\n\u001b[0m\u001b[0;32m   1431\u001b[0m       value, dtype=dtype, dtype_hint=dtype_hint, name=name)\n\u001b[0;32m   1432\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor_v2\u001b[1;34m(value, dtype, dtype_hint, name)\u001b[0m\n\u001b[0;32m   1434\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mconvert_to_tensor_v2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype_hint\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1435\u001b[0m   \u001b[1;34m\"\"\"Converts the given `value` to a `Tensor`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1436\u001b[1;33m   return convert_to_tensor(\n\u001b[0m\u001b[0;32m   1437\u001b[0m       \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1438\u001b[0m       \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\profiler\\trace.py\u001b[0m in \u001b[0;36mwrapped\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    161\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrace_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mtrace_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m           \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mconvert_to_tensor\u001b[1;34m(value, dtype, name, as_ref, preferred_dtype, dtype_hint, ctx, accepted_result_types)\u001b[0m\n\u001b[0;32m   1564\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1565\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1566\u001b[1;33m       \u001b[0mret\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconversion_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1567\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1568\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNotImplemented\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\tensor_conversion_registry.py\u001b[0m in \u001b[0;36m_default_conversion_function\u001b[1;34m(***failed resolving arguments***)\u001b[0m\n\u001b[0;32m     50\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_default_conversion_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ref\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     51\u001b[0m   \u001b[1;32mdel\u001b[0m \u001b[0mas_ref\u001b[0m  \u001b[1;31m# Unused.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 52\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mconstant_op\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     53\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconstant\u001b[1;34m(value, dtype, shape, name)\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[0mValueError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mcalled\u001b[0m \u001b[0mon\u001b[0m \u001b[0ma\u001b[0m \u001b[0msymbolic\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    270\u001b[0m   \"\"\"\n\u001b[1;32m--> 271\u001b[1;33m   return _constant_impl(value, dtype, shape, name, verify_shape=False,\n\u001b[0m\u001b[0;32m    272\u001b[0m                         allow_broadcast=True)\n\u001b[0;32m    273\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_impl\u001b[1;34m(value, dtype, shape, name, verify_shape, allow_broadcast)\u001b[0m\n\u001b[0;32m    281\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"tf.constant\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    282\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 283\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    284\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    285\u001b[0m   \u001b[0mg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_default_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36m_constant_eager_impl\u001b[1;34m(ctx, value, dtype, shape, verify_shape)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0m_constant_eager_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverify_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    307\u001b[0m   \u001b[1;34m\"\"\"Creates a constant on the current device.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 308\u001b[1;33m   \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconvert_to_eager_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    309\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mshape\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    310\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py\u001b[0m in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    104\u001b[0m       \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_dtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_datatype_enum\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    105\u001b[0m   \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 106\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEagerTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdevice_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    107\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from sklearn.model_selection import KFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.applications.mobilenet_v2 import preprocess_input\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "#Parameters\n",
    "num_classes = 7\n",
    "img_width, img_height = 48, 48 #Resize all images to this size. Ensure minimum resolution is not less than this.\n",
    "min_res = (img_width, img_height) # Update to set minimum resolution of photos to train on \n",
    "max_res = (512, 512) # Update to set maximum resolution of photos to train on \n",
    "\n",
    "def load_data(data_dir, min_resolution=None, max_resolution=None):\n",
    "    images = []\n",
    "    labels = []\n",
    "    emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\",\"surprise\"]\n",
    "    \n",
    "    for idx, emotion in enumerate(emotions):\n",
    "        emotion_folder = os.path.join(data_dir, emotion)\n",
    "        for img_file in os.listdir(emotion_folder):\n",
    "            img_path = os.path.join(emotion_folder, img_file)\n",
    "            image = cv2.imread(img_path)\n",
    "            height, width, _ = image.shape\n",
    "            #img = load_img(img_path, color_mode='rgb', target_size=(128, 128))  # Load image in RGB\n",
    "            #img_gray = cv2.cvtColor(img_to_array(img), cv2.COLOR_RGB2GRAY)  # Convert to grayscale\n",
    "            #img_rgb_gray = cv2.cvtColor(img_gray, cv2.COLOR_GRAY2RGB)  # Convert back to RGB\n",
    "            \n",
    "            # Check image resolution\n",
    "            if (min_resolution is None or (height >= min_resolution[0] and width >= min_resolution[1])) and \\\n",
    "               (max_resolution is None or (height <= max_resolution[0] and width <= max_resolution[1])):\n",
    "                image = cv2.resize(image, (img_width, img_height))\n",
    "                images.append(image)\n",
    "                labels.append(idx)\n",
    "    return np.array(images), np.array(labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Load and preprocess data\n",
    "data_directory = r'C:\\Important Files\\Python Projects\\Facial Emotion Recognition\\FER-2013\\dataset'  # Replace this with your dataset directory\n",
    "X, y = load_data(data_directory, min_resolution=min_res, max_resolution=max_res)\n",
    "X = X / 255.0  # Normalize pixel values\n",
    "\n",
    "# Convert labels to categorical\n",
    "y = to_categorical(y, num_classes=num_classes)  # 7 classes for the 7 emotions\n",
    "\n",
    "# Define the hyperparameters grid for grid search\n",
    "#Best hyperparameters: {'batch_size': 16, 'dropout_val': 0.3, 'epochs': 5, 'l2_val': 0.01, 'num_folds': 10}\n",
    "param_grid = {\n",
    "    'batch_size': [16, 32, 64],\n",
    "    'l2_val': [0.1, 0.01, 0.001],\n",
    "    'dropout_val': [0.7, 0.5, 0.3],\n",
    "    'epochs': [5, 10, 15],\n",
    "    'num_folds': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Define the CNN model with Ridge regularization\n",
    "def create_model(l2_val, dropout_val):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', input_shape=(img_width, img_height, 3), kernel_regularizer=l2(l2_val)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_regularizer=l2(l2_val)))\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_regularizer=l2(l2_val)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(7, activation='softmax'))  # Output layer with 7 classes\n",
    "    model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "best_params = {}\n",
    "\n",
    "# Perform grid search\n",
    "for params in ParameterGrid(param_grid):\n",
    "    print(f\"Training with hyperparameters: {params}\")\n",
    "    \n",
    "    batch_size = params['batch_size']\n",
    "    l2_val = params['l2_val']\n",
    "    dropout_val = params['dropout_val']\n",
    "    epochs = params['epochs']\n",
    "    num_folds = params['num_folds']\n",
    "    \n",
    "    # Perform K-fold cross-validation\n",
    "    kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "    fold_no = 1\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X, y):\n",
    "        model = create_model(l2_val, dropout_val)\n",
    "        X_train, X_val = X[train_idx], X[val_idx]\n",
    "        y_train, y_val = y[train_idx], y[val_idx]\n",
    "        \n",
    "        checkpoint_path ='emotional_CNN_val_accuracy_{val_accuracy:.2f}.h5'\n",
    "        checkpoint = ModelCheckpoint(checkpoint_path,\n",
    "                                     monitor='val_accuracy', verbose=1,\n",
    "                                     save_best_only=True, mode='max')\n",
    "        \n",
    "        history = model.fit(X_train, y_train, epochs=epochs, batch_size=batch_size,\n",
    "                            validation_data=(X_val, y_val), callbacks=[checkpoint], verbose=1)\n",
    "        \n",
    "        val_acc = max(history.history['val_accuracy'])\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            best_epoch = np.argmax(history.history['val_accuracy']) + 1\n",
    "            best_params = params\n",
    "    \n",
    "        fold_no += 1\n",
    "\n",
    "print(f\"Best validation accuracy: {best_val_acc} at epoch {best_epoch}\")\n",
    "print(\"Best hyperparameters:\", best_params)\n",
    "\n",
    "\n",
    "\n",
    "# Now you can deploy the best model to recognize emotions from your camera\n",
    "# Load the best model using model = load_model('best_model.h5') and perform predictions on live camera feed\n",
    "# Remember to preprocess the camera feed similar to how the training data was preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live feed preprocessing script should be implemented to process the live camera frames using OpenCV\n",
    "# and feed them into the trained model for inference\n",
    "# This involves capturing frames from the camera, preprocessing them similarly to the training data,\n",
    "# and performing inference using the trained model.\n",
    "# You can use OpenCV's VideoCapture to access the live camera feed.\n",
    "\n",
    "# Load the trained model\n",
    "model = load_model('emotional_CNN_val_accuracy_0.52.h5')  # Load your best model here\n",
    "\n",
    "# Define emotions for mapping the output\n",
    "emotions = [\"angry\", \"disgust\", \"fear\", \"happy\", \"neutral\", \"sad\",\"surprise\"]\n",
    "\n",
    "# Function to preprocess live feed frames\n",
    "def preprocess_live_frame(frame):\n",
    "    # Resize the frame to match the input size of the trained model\n",
    "    resized_frame = cv2.resize(frame, (img_width, img_height))\n",
    "    frame_normalized = resized_frame / 255.0  # Normalize pixel values\n",
    "    return np.expand_dims(frame_normalized, axis=0)\n",
    "\n",
    "# Access the live camera feed\n",
    "cap = cv2.VideoCapture(0)  # Use 0 for the default camera, you may need to change this number\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Preprocess the live frame\n",
    "    processed_frame = preprocess_live_frame(frame)\n",
    "    \n",
    "    # Perform inference using the trained model\n",
    "    predictions = model.predict(processed_frame)\n",
    "    confidence = np.max(predictions)\n",
    "    emotion_label = emotions[np.argmax(predictions)]\n",
    "    \n",
    "    # Display recognized emotion and confidence level on the camera feed\n",
    "    text = f\"Emotion: {emotion_label}, Confidence: {confidence:.2f}\"\n",
    "    cv2.putText(frame, text, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Show the camera feed with emotion and confidence\n",
    "    cv2.imshow('Emotion Recognition', frame)\n",
    "    \n",
    "    # Exit loop if 'q' is pressed\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# Release the camera and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
